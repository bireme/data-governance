"""
# Data Governance - Tasks for 01 DAG

Task reutiliz치vel para coleta e armazenamento de dados do FI-Admin no MongoDB.

## Objetivo Principal
Coordenar a ingest칚o segura e eficiente de dados do sistema FI-Admin para a camada de landing zone no MongoDB, com tratamento de dados e monitoramento integrado.

## 游댐 Funcionalidades-Chave
1. **Coleta Resiliente**
   - Pagina칞칚o autom치tica (limit/offset)
   - Retry inteligente para falhas HTTP (5 tentativas com backoff)
   - Tratamento especial para erro 502 (continua칞칚o autom치tica)

2. **Gest칚o de Dados**
   - Detec칞칚o e preven칞칚o de duplicidades
   - Modos de atualiza칞칚o configur치veis (FULL/INCREMENTAL)
   - Inser칞칚o em lote

## 丘뙖잺 Par칙metros de Configura칞칚o
| Par칙metro        | Tipo    | Default  | Descri칞칚o                                                                 |
|------------------|---------|----------|---------------------------------------------------------------------------|
| `update_mode`    | string  | REQUIRED | Modo de opera칞칚o: `FULL` (carga completa) ou `INCREMENTAL` (5 dias atr치s) |
| `mongo_conn_id`  | string  | 'mongo'  | Conex칚o Airflow para MongoDB                                             |
| `fiadmin_conn_id`| string  | 'fiadmin'| Conex칚o Airflow com host da API e apikey no password        
"""


import requests
import pymongo
import logging
import time
import math
from datetime import datetime
from datetime import timedelta
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from airflow.providers.mongo.hooks.mongo import MongoHook
from airflow.hooks.base import BaseHook
from airflow.decorators import task


class Timer:
    def __enter__(self):
        self.start = time.perf_counter()
        return self  # permite acessar atributos depois
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end = time.perf_counter()
        self.interval = self.end - self.start


def get_requests_session():
    session = requests.Session()
    retries = Retry(
        total=5,  # Number of total attempts
        backoff_factor=2,  # Wait 1s, 2s, 4s between retries
        status_forcelist=[429, 500, 503, 504],  # Retry on these status codes
    )
    adapter = HTTPAdapter(max_retries=retries)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session


# Fun칞칚o para obter JSON da API de forma paginada e enviar para o MongoDB
@task
def harvest_fiadmin_and_store_in_mongodb(update_mode):
    logger = logging.getLogger(__name__)

    fiadmin_conn = BaseHook.get_connection('fiadmin')

    mongo_hook = MongoHook(mongo_conn_id='mongo')
    mongo_client = mongo_hook.get_conn()
    mongo_db = mongo_client["data_governance"]
    mongo_collection = mongo_db["01_landing_zone"]

    url = fiadmin_conn.host
    limit = 100
    offset = 0
    status = 1
    extra_params = {}
    headers = {'apikey': fiadmin_conn.password}
    session = get_requests_session()

    if update_mode == "INCREMENTAL":
        incremental_update_date = (datetime.today() - timedelta(days=5)).strftime('%Y-%m-%d')
        extra_params = {'updated_time__gte': incremental_update_date}
    elif update_mode == "FULL":
        total_records = mongo_collection.count_documents({})
        offset = math.floor(total_records / limit) * limit
        logger.info(f"H치 {total_records} registros na cole칞칚o. Ativando offset {offset}.")
    
    has_more_data = True
    while has_more_data:
        params = {
            "limit": limit,
            "offset": offset,
            "status": status,
            "format": "json",
            **extra_params
        }
        logger.info(f"Coletando offset {offset} no FI-Admin com os parametros {params}")
        with Timer() as t:
            response = session.get(url, params=params, headers=headers)
        logger.info(f"Tempo de coleta de dados no FI-Admin: {t.interval:.4f} segundos")

        if response.status_code == 200:
            json_data = response.json()
            
            # Se a resposta estiver vazia, n칚o h치 mais dados
            if not json_data or (json_data and not json_data['objects']):
                has_more_data = False
            else:
                results = json_data['objects']
                with Timer() as t:
                    send_json_to_mongodb(results, mongo_collection)
                logger.info(f"Tempo de inser칞칚o de dados no MongoDB: {t.interval:.4f} segundos")
                
                offset += limit
        elif response.status_code == 502:
            logger.error(f"Erro {response.status_code} na coleta do batch de offset {offset} e limit {limit}. Pulando para o pr칩ximo batch.")
            offset += limit
        else:
            raise Exception(f"Erro ao obter dados do FI-Admin: Status code {response.status_code}")
            has_more_data = False


# Fun칞칚o para enviar JSON para o MongoDB
def send_json_to_mongodb(json_data, collection):
    # Extrai todos os IDs do lote atual
    batch_ids = [doc['id'] for doc in json_data if 'id' in doc]
    
    # Consulta os IDs existentes no MongoDB
    existing_ids = collection.distinct('id', {'id': {'$in': batch_ids}})
    
    # Filtra documentos novos
    new_docs = [
        doc for doc in json_data 
        if doc.get('id') not in existing_ids
    ]

    if new_docs:
        try:
            collection.insert_many(new_docs, ordered=False)
        except pymongo.errors.BulkWriteError as e:
            # Check if the error is a duplicate key error
            for err in e.details['writeErrors']:
                if err['code'] == 11000:  # Duplicate key error code
                    # Ignore this error
                    continue
                else:
                    # Handle other errors
                    raise Exception(f"Error code {err['code']}: {err['errmsg']}")